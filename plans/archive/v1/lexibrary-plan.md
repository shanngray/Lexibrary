# Lexibrary / Lexibrarian — Implementation Plan

## Context

Build a Python utility that lives inside a software project's root directory. It crawls the folder tree and creates `.aindex` (Markdown) index files in each directory. These files give AI agents a quick map of the codebase — file summaries and token counts — so they can navigate without reading every file, conserving context window budget.

**Key decisions:**
- Python, packaged with `uv` (pyproject.toml + hatchling)
- Configurable multi-provider LLM (Anthropic, OpenAI, Ollama)
- Configurable tokenizer (tiktoken, Anthropic API, approximate fallback)
- **BAML** for all LLM prompt definitions (typed, structured, provider-agnostic)
- CLI (`lexibrarian init|crawl|daemon|status|clean`) via Typer — aliased as both `lexibrarian` and `lexi`
- Daemon mode: watchdog file watcher + debounce + periodic full sweep
- Respects `.gitignore` patterns (via `pathspec`)
- Per-file token counts in `.aindex`
- **OpenSpec** (globally installed) — run `openspec init` when ready for spec-driven development

---

## Project Structure

```
Lexibrarian/
├── pyproject.toml
├── .python-version              # e.g. 3.12
├── .gitignore
├── README.md
├── baml_src/                        # BAML prompt definitions
│   ├── clients.baml                 # LLM client configs (Anthropic, OpenAI, Ollama)
│   ├── summarize_file.baml          # File summarization function
│   ├── summarize_files_batch.baml   # Batch file summarization function
│   └── summarize_directory.baml     # Directory summarization function
├── src/
│   └── lexibrarian/
│       ├── __init__.py
│       ├── __main__.py          # python -m lexibrarian
│       ├── cli.py               # Typer CLI app
│       ├── baml_client/             # Auto-generated by `baml-cli generate` (do not edit)
│       ├── config/
│       │   ├── __init__.py
│       │   ├── schema.py        # Pydantic models for lexibrary.toml
│       │   ├── loader.py        # Find & load config
│       │   └── defaults.py      # Default config values
│       ├── crawler/
│       │   ├── __init__.py
│       │   ├── engine.py        # Core crawl orchestrator (bottom-up)
│       │   ├── discovery.py     # Dir/file discovery with ignore filtering
│       │   ├── change_detector.py  # Hash-based change detection + cache
│       │   └── file_reader.py   # Read files, detect binary, handle large files
│       ├── indexer/
│       │   ├── __init__.py
│       │   ├── generator.py     # Generate .aindex Markdown content
│       │   ├── writer.py        # Atomic file writes
│       │   └── parser.py        # Parse existing .aindex files
│       ├── llm/
│       │   ├── __init__.py
│       │   ├── service.py       # Thin wrapper calling baml_client functions
│       │   ├── factory.py       # Config → BAML client registry setup
│       │   └── rate_limiter.py  # Token-bucket rate limiter
│       ├── tokenizer/
│       │   ├── __init__.py
│       │   ├── base.py          # TokenCounter Protocol
│       │   ├── tiktoken_counter.py
│       │   ├── anthropic_counter.py
│       │   ├── approximate.py   # chars/4 fallback
│       │   └── factory.py
│       ├── daemon/
│       │   ├── __init__.py
│       │   ├── watcher.py       # watchdog event handler
│       │   ├── debouncer.py     # Coalesce rapid FS events
│       │   ├── scheduler.py     # Periodic full sweep timer
│       │   └── service.py       # Daemon lifecycle + signal handling
│       ├── ignore/
│       │   ├── __init__.py
│       │   ├── gitignore.py     # .gitignore parsing via pathspec
│       │   ├── patterns.py      # Config-based extra patterns
│       │   └── matcher.py       # Combined ignore matcher
│       └── utils/
│           ├── __init__.py
│           ├── hashing.py       # SHA-256 file hashing
│           ├── logging.py       # Logging config
│           └── paths.py         # Path helpers
├── tests/
│   ├── conftest.py
│   ├── test_config/
│   ├── test_crawler/
│   ├── test_indexer/
│   ├── test_llm/
│   ├── test_tokenizer/
│   ├── test_daemon/
│   ├── test_ignore/
│   ├── test_cli.py
│   └── fixtures/
│       └── sample_project/
└── docs/
```

---

## .aindex File Format

```markdown
# dirname/

1-3 sentence summary of the directory's purpose.

## Files

| File | Tokens | Description |
|------|--------|-------------|
| `cli.py` | 534 | Typer CLI with commands: init, crawl, daemon, status. |
| `__init__.py` | 32 | Package version and top-level exports. |

## Subdirectories

| Directory | Description |
|-----------|-------------|
| `config/` | Configuration loading and validation using Pydantic. |
| `crawler/` | Core crawl engine: bottom-up traversal, change detection. |
```

Rules: H1 = dir name with trailing `/`. Summary = plain text, no markup. Files table sorted alphabetically, token count is integer, description is one sentence. Subdirectories table pulls summary from child `.aindex`. Sections show `(none)` if empty.

---

## Configuration: `lexibrary.toml`

```toml
[llm]
provider = "anthropic"                    # anthropic | openai | ollama
model = "claude-sonnet-4-5-20250514"
api_key_env = "ANTHROPIC_API_KEY"         # env var name
max_retries = 3
timeout = 60

[tokenizer]
backend = "tiktoken"                      # tiktoken | anthropic_api | approximate
model = "cl100k_base"

[crawl]
root = "."
max_file_size_kb = 512
max_files_per_llm_batch = 10
summary_max_tokens = 80
dir_summary_max_tokens = 150
binary_extensions = [".png", ".jpg", ".gif", ".zip", ".exe", ".pdf", ".pyc", ".sqlite", "..."]

[ignore]
use_gitignore = true
additional_patterns = [".aindex", "lexibrary.toml", ".env", "*.lock", "__pycache__/", ".git/", ".venv/"]

[daemon]
debounce_seconds = 2.0
full_sweep_interval_minutes = 30
log_file = ".lexibrarian.log"

[output]
filename = ".aindex"
include_token_counts = true
```

Config validated via Pydantic models. Loader walks upward from CWD to find `lexibrary.toml`.

---

## Core Architecture

### Crawl Engine (bottom-up)

1. Build ignore matcher (`.gitignore` + config patterns)
2. `os.walk(topdown=False)` → directories sorted deepest-first
3. For each directory:
   - List files (filter ignored, binary, oversized)
   - Load existing `.aindex` → cached entries
   - For each file: compute SHA-256 hash
     - Hash matches cache → reuse summary + token count
     - Hash differs → read file → count tokens → LLM summarize
   - Batch small changed files into single LLM call (JSON response)
   - Read child subdirectory `.aindex` summaries
   - LLM summarize directory (given file list + subdir summaries)
   - Generate + atomically write `.aindex`
   - Update `.lexibrarian_cache.json`

### LLM Integration via BAML

All prompts are defined in `.baml` files under `baml_src/`. BAML compiles to a type-safe `baml_client/` Python module. Provider switching is handled by BAML's client registry — no custom provider abstraction needed.

**BAML functions:**
- `SummarizeFile(filename, language, content, is_truncated) → FileSummary`
- `SummarizeFilesBatch(files) → list[FileSummary]`
- `SummarizeDirectory(dirname, file_entries, subdir_entries) → string`

**BAML clients (in `clients.baml`):**
- `AnthropicClient` — `anthropic/claude-sonnet-4-5-20250514` (default)
- `OpenAIClient` — `openai/gpt-4o-mini`
- `OllamaClient` — custom base_url `http://localhost:11434`

The `llm/factory.py` module reads `lexibrary.toml` and registers the correct BAML client at runtime via the client registry. The `llm/service.py` module provides a thin async wrapper that calls `baml_client` functions with rate limiting.

**Workflow:** `baml-cli generate` → generates `baml_client/` → Python code imports from it.

### Cost-saving strategies

- Batch small files into single LLM calls
- Skip binary files entirely (listed as "Binary file (type)")
- Incremental: only re-summarize changed files (hash-based)
- Low `max_tokens` on responses (summaries are 1 sentence)
- Truncate large files before LLM, include truncation notice in prompt
- Support cheap models (haiku, gpt-4o-mini) via config

### Daemon

- **watchdog Observer** → detects FS events
- **Debouncer** → coalesces rapid changes per-directory (configurable delay)
- **Periodic sweep** → full crawl every N minutes as safety net
- **PID file** + signal handlers for graceful shutdown
- Incremental re-index: affected dirs + their ancestors up to root

---

## BAML Prompt Definitions

### `baml_src/clients.baml`

```baml
client<llm> AnthropicClient {
  provider "anthropic"
  options {
    model "claude-sonnet-4-5-20250514"
    api_key env.ANTHROPIC_API_KEY
    max_tokens 150
  }
}

client<llm> OpenAIClient {
  provider "openai"
  options {
    model "gpt-4o-mini"
    api_key env.OPENAI_API_KEY
    max_tokens 150
  }
}

client<llm> OllamaClient {
  provider "openai"
  options {
    model "llama3.2"
    base_url "http://localhost:11434/v1"
    api_key "ollama"
    max_tokens 150
  }
}
```

### `baml_src/summarize_file.baml`

```baml
class FileSummary {
  summary string @description("One concise sentence describing what the file does and why it exists")
}

function SummarizeFile(filename: string, language: string, content: string, is_truncated: bool) -> FileSummary {
  client AnthropicClient
  prompt #"
    You are a code documentation assistant. Write an extremely concise file summary (one sentence, max 80 tokens).
    Focus on WHAT the file does and WHY it exists. Use imperative style.
    Never start with "This file".

    File: {{ filename }}
    Language: {{ language }}
    {% if is_truncated %}(file truncated){% endif %}
    ---
    {{ content }}
    ---

    {{ ctx.output_format }}
  "#
}
```

### `baml_src/summarize_files_batch.baml`

```baml
class FileEntry {
  file string
  content string
  language string
}

class BatchFileSummary {
  file string
  summary string
}

function SummarizeFilesBatch(files: FileEntry[]) -> BatchFileSummary[] {
  client AnthropicClient
  prompt #"
    For each file below, write exactly ONE concise sentence summary (max 80 tokens each).
    Focus on WHAT each file does and WHY. Use imperative style.

    {% for f in files %}
    === {{ f.file }} ({{ f.language }}) ===
    {{ f.content }}

    {% endfor %}

    {{ ctx.output_format }}
  "#
}
```

### `baml_src/summarize_directory.baml`

```baml
function SummarizeDirectory(dirname: string, file_list: string, subdir_list: string) -> string {
  client AnthropicClient
  prompt #"
    Write a 1-3 sentence summary of this directory's purpose and role in the project.
    Be concise (max 150 tokens). Plain text, no markup.

    Directory: {{ dirname }}

    Files:
    {{ file_list }}

    Subdirectories:
    {{ subdir_list }}
  "#
}
```

At runtime, `llm/factory.py` reads the `provider` from `lexibrary.toml` and uses BAML's client registry to switch the active client (e.g., swap `AnthropicClient` for `OllamaClient`).

---

## Dependencies

**Core:** typer, rich, pydantic, pathspec, watchdog, tiktoken, baml-py, httpx, tomli-w
**Optional:** ollama
**Dev:** pytest, pytest-asyncio, pytest-cov, ruff, mypy, respx, openspec (global npm)

Note: `baml-py` replaces direct `anthropic`/`openai` SDK dependencies — BAML handles provider SDKs internally. Run `baml-cli generate` after editing any `.baml` file.

---

## CLI Commands

Both `lexibrarian` and `lexi` work as CLI entry points (registered in `pyproject.toml` `[project.scripts]`).

| Command | Description |
|---------|-------------|
| `lexi init [path]` | Create `lexibrary.toml`, add `.aindex` to `.gitignore` |
| `lexi crawl [path] [--full] [--dry-run] [-v]` | Run full/incremental crawl with rich progress |
| `lexi daemon [path] [--foreground]` | Start background watcher + periodic sweep |
| `lexi status [path]` | Show index stats, stale files, daemon status |
| `lexi clean [path] [--yes]` | Remove all `.aindex` files + cache |

---

## Implementation Phases

### Phase 1: Foundation
Scaffolding, config system, ignore matcher, utils (hashing, logging, paths).
**Files:** pyproject.toml, config/*, ignore/*, utils/*
**Milestone:** `uv run python -m lexibrarian --help` works.

### Phase 2: Token Counting
TokenCounter protocol, tiktoken + approximate backends, factory.
**Files:** tokenizer/*
**Milestone:** Can count tokens in any file.

### Phase 3: .aindex Format
Generator, atomic writer, parser (for incremental reuse).
**Files:** indexer/*
**Milestone:** Round-trip generate → parse works.

### Phase 4: LLM Integration (BAML)
Write BAML prompt definitions, generate `baml_client`, build thin service wrapper + rate limiter.
**Files:** baml_src/*.baml, llm/service.py, llm/factory.py, llm/rate_limiter.py
**Milestone:** `baml-cli generate` succeeds. Can summarize a file via any configured provider.

### Phase 5: Crawler Engine
File reader, bottom-up discovery, change detector (cache), crawl orchestrator.
**Files:** crawler/*
**Milestone:** `lexibrarian crawl` produces `.aindex` files for a project.

### Phase 6: CLI
Wire up all commands with rich output and progress bars.
**Files:** cli.py, __main__.py
**Milestone:** All CLI commands work end-to-end.

### Phase 7: Daemon
Watchdog watcher, debouncer, periodic scheduler, service lifecycle.
**Files:** daemon/*
**Milestone:** `lexibrarian daemon` watches and incrementally updates.

### Phase 8: Polish
Error hardening, logging, README, tests with coverage targets.

---

## Verification

1. `uv run lexi init` → creates `lexibrary.toml`
2. `uv run lexi crawl --dry-run` → shows what would be indexed
3. `uv run lexi crawl` on a small test project → `.aindex` files appear in every directory
4. Modify a file → `uv run lexi crawl` → only changed file re-summarized (check logs)
5. `uv run lexi status` → shows correct counts
6. `uv run lexi daemon --foreground` → modify a file → `.aindex` updates within debounce window
7. `uv run lexi clean --yes` → all `.aindex` files removed
8. `uv run pytest --cov` → tests pass with reasonable coverage
9. `baml-cli generate` → `baml_client/` regenerates without errors
