# Lexibrarian — Start Here

> **Note:** `blueprints/` is a hand-maintained pseudo-lexibrary for agents building Lexibrarian itself — not to be confused with the `.lexibrary/` output that Lexibrarian produces for other projects.

## Project Topology

```
src/lexibrarian/
├── __init__.py
├── __main__.py
├── cli.py                       ← Typer CLI entry point (lexi command)
├── exceptions.py                ← LexibraryNotFoundError
├── search.py                    ← unified_search() — cross-artifact search (concepts, design files, Stack posts)
├── archivist/                   ← LLM pipeline for design file + START_HERE generation (Phase 4)
│   ├── __init__.py              ← Public API re-exports
│   ├── change_checker.py        ← ChangeLevel enum + check_change() — classify source vs design file drift
│   ├── dependency_extractor.py  ← Tree-sitter import resolution to project-relative paths
│   ├── pipeline.py              ← update_file() + update_project() — full generation pipeline
│   ├── service.py               ← ArchivistService — async BAML calls with provider routing
│   └── start_here.py            ← generate_start_here() — project-level START_HERE.md generation
├── artifacts/                   ← Pydantic 2 models + parser/serializer/writer for all artifact types
│   ├── __init__.py
│   ├── aindex.py                ← AIndexFile + AIndexEntry models
│   ├── aindex_parser.py         ← Parse .aindex files from disk
│   ├── aindex_serializer.py     ← Serialize AIndexFile to .aindex text
│   ├── concept.py               ← ConceptFile + ConceptFileFrontmatter models
│   ├── design_file.py           ← DesignFile + DesignFileFrontmatter + StalenessMetadata models
│   ├── design_file_parser.py    ← Parse design files from disk (full, metadata-only, frontmatter-only)
│   ├── design_file_serializer.py ← Serialize DesignFile to YAML frontmatter + markdown + footer
│   └── writer.py                ← write_artifact() — persists any artifact to disk
├── ast_parser/                  ← Tree-sitter interface extraction: models, registry, language parsers, canonical renderer
│   ├── __init__.py              ← Public API: parse_interface(), hash_interface(), compute_hashes()
│   ├── models.py                ← Pydantic 2 models: InterfaceSkeleton, ClassSig, FunctionSig, ConstantSig, ParameterSig
│   ├── registry.py              ← GrammarInfo + GRAMMAR_MAP; lazy-load/cache tree-sitter Language + Parser
│   ├── skeleton_render.py       ← render_skeleton() — deterministic canonical text for hashing
│   ├── python_parser.py         ← extract_interface() for .py / .pyi
│   ├── typescript_parser.py     ← extract_interface() for .ts / .tsx
│   └── javascript_parser.py     ← extract_interface() for .js / .jsx
├── baml_client/                 ← AUTO-GENERATED by BAML — do not edit; contents omitted from this tree
├── config/                      ← Config schema (Pydantic 2) + loader + defaults
│   ├── __init__.py
│   ├── defaults.py              ← Default config template written on lexi init
│   ├── loader.py                ← Two-tier YAML loader (project + user overrides)
│   └── schema.py                ← LexibraryConfig Pydantic model
├── crawler/                     ← Bottom-up crawl orchestration + change detection (LLM-based, partially broken)
│   ├── __init__.py
│   ├── change_detector.py       ← SHA-256 + LLM-based change detection
│   ├── discovery.py             ← Filesystem traversal + file enumeration
│   ├── engine.py                ← full_crawl() orchestrator (partially broken — see blueprints)
│   └── file_reader.py           ← Read source files for crawl input
├── daemon/                      ← Watchdog file watcher + debouncer + periodic sweep
│   ├── __init__.py
│   ├── debouncer.py             ← Coalesce rapid file-change events
│   ├── scheduler.py             ← Periodic sweep scheduler
│   ├── service.py               ← DaemonService — top-level watchdog coordinator
│   └── watcher.py               ← watchdog filesystem event handler
├── ignore/                      ← pathspec-based ignore pattern matching
│   ├── __init__.py
│   ├── gitignore.py             ← Load + parse .gitignore files
│   ├── matcher.py               ← IgnoreMatcher combining .gitignore + config + .lexignore patterns
│   └── patterns.py              ← Built-in default ignore patterns
├── indexer/                     ← Structural .aindex generation (no LLM): generator + orchestrator
│   ├── __init__.py
│   ├── generator.py             ← generate_aindex() — builds AIndexFile from a directory
│   └── orchestrator.py          ← lexi index entry point; drives generator → serializer → writer
├── init/                        ← lexi init scaffolding (.lexibrary/ skeleton)
│   ├── __init__.py
│   └── scaffolder.py            ← create_lexibrary_skeleton()
├── llm/                         ← BAML client wrapper + rate limiter + factory
│   ├── __init__.py
│   ├── factory.py               ← create_llm_service() factory
│   ├── rate_limiter.py          ← Token-bucket rate limiter
│   └── service.py               ← LLMService wrapping BAML async client
├── stack/                       ← Stack Overflow-style Q&A knowledge base (Phase 6)
│   ├── __init__.py              ← Public API re-exports
│   ├── models.py                ← StackPost, StackPostFrontmatter, StackAnswer, StackPostRefs models
│   ├── index.py                 ← StackIndex — in-memory search/filter of Stack posts
│   ├── mutations.py             ← add_answer(), record_vote(), accept_answer(), mark_duplicate(), mark_outdated()
│   ├── parser.py                ← parse_stack_post() — YAML frontmatter + markdown → StackPost
│   ├── serializer.py            ← serialize_stack_post() — StackPost → markdown string
│   └── template.py              ← render_post_template() — scaffold new Stack post files
├── tokenizer/                   ← Pluggable token counting backends
│   ├── __init__.py
│   ├── anthropic_counter.py     ← Anthropic token counting backend
│   ├── approximate.py           ← Character-ratio approximate backend
│   ├── base.py                  ← TokenCounter protocol
│   ├── factory.py               ← Backend selection factory
│   └── tiktoken_counter.py      ← tiktoken (OpenAI) backend
├── utils/                       ← Hashing, language detection, logging, paths, root
│   ├── __init__.py
│   ├── hashing.py               ← hash_file() SHA-256 helper
│   ├── languages.py             ← detect_language() by extension
│   ├── logging.py               ← setup_logging() rich handler
│   ├── paths.py                 ← Path helpers for .lexibrary/ layout
│   └── root.py                  ← find_project_root() — walks up to locate .lexibrary/
└── wiki/                        ← Concept file parser, serializer, template, resolver, and index (Phase 5)
    ├── __init__.py              ← Public API re-exports
    ├── index.py                 ← ConceptIndex — in-memory search/retrieval of concept files
    ├── parser.py                ← parse_concept_file() — markdown + YAML frontmatter → ConceptFile
    ├── resolver.py              ← WikilinkResolver — resolve [[wikilinks]] to concepts or Stack posts
    ├── serializer.py            ← serialize_concept_file() — ConceptFile → markdown string
    └── template.py              ← render_concept_template(), concept_file_path() — scaffolding helpers
```

## Package Map

| Package | Role |
| --- | --- |
| `archivist` | LLM pipeline for design file + START_HERE generation: `ArchivistService`, `update_file`, `update_project`, `generate_start_here`, `check_change`, `extract_dependencies` |
| `artifacts` | Pydantic 2 models: `DesignFile`, `AIndexFile`, `ConceptFile`; plus parsers, serializers, writer |
| `ast_parser` | Tree-sitter interface extraction: `parse_interface`, `compute_hashes`, `hash_interface`; `InterfaceSkeleton` model; Python / TypeScript / JavaScript parsers; `render_skeleton` canonical renderer |
| `config` | `LexibraryConfig` schema (incl. `scope_root`, `ASTConfig`), two-tier YAML loader, default config template |
| `crawler` | `full_crawl()` orchestrator; discovery, file reading, change detection (LLM-based; partially broken -- see crawler/engine.md) |
| `daemon` | `DaemonService` -- watchdog + debounce + periodic sweep |
| `ignore` | `IgnoreMatcher` combining `.gitignore` + config + `.lexignore` patterns via pathspec |
| `indexer` | Structural `.aindex` pipeline: `generate_aindex` (with design file frontmatter lookup) -> `serialize_aindex` -> `write_artifact`; no LLM |
| `init` | `create_lexibrary_skeleton()` -- creates `.lexibrary/` + `.lexignore` on `lexi init` |
| `llm` | `LLMService` wrapping BAML client; `RateLimiter`; `create_llm_service()` factory |
| `stack` | Stack Q&A knowledge base: `StackPost`, `StackIndex`, `parse_stack_post`, `serialize_stack_post`, `render_post_template`; mutations: `add_answer`, `record_vote`, `accept_answer`, `mark_duplicate`, `mark_outdated` |
| `tokenizer` | `TokenCounter` protocol; tiktoken / anthropic / approximate backends |
| `utils` | `hash_file`, `detect_language`, `setup_logging`, `find_project_root`, path helpers |
| `wiki` | `ConceptIndex` (search/retrieval by title, alias, tag, substring); `parse_concept_file`; `serialize_concept_file`; `WikilinkResolver` (`ResolvedLink`, `UnresolvedLink`); `render_concept_template`, `concept_file_path` |
| `search` | `unified_search()` — cross-artifact search across concepts, design files, and Stack posts; `SearchResults` with Rich rendering |

## Navigation by Intent

| Task | Read first |
| --- | --- |
| Add / modify a CLI command | `blueprints/src/lexibrarian/cli.md` |
| Modify design file generation pipeline | `blueprints/src/lexibrarian/archivist/pipeline.md` |
| Change archivist LLM service or provider routing | `blueprints/src/lexibrarian/archivist/service.md` |
| Change change detection logic | `blueprints/src/lexibrarian/archivist/change_checker.md` |
| Modify START_HERE generation | `blueprints/src/lexibrarian/archivist/start_here.md` |
| Add a language parser or modify AST extraction | `blueprints/src/lexibrarian/ast_parser/` |
| Change config keys or defaults | `blueprints/src/lexibrarian/config/` |
| Modify crawl logic (LLM-based) | `blueprints/src/lexibrarian/crawler/engine.md` |
| Modify structural indexing (no LLM) | `blueprints/src/lexibrarian/indexer/` |
| Change ignore patterns or .lexignore | `blueprints/src/lexibrarian/ignore/` |
| Add / modify LLM prompts | `baml_src/` (source-of-truth for prompts) |
| Change artifact data models | `blueprints/src/lexibrarian/artifacts/` |
| Change `.aindex` file format | `blueprints/src/lexibrarian/artifacts/aindex_serializer.md` + `aindex_parser.md` |
| Change design file format | `blueprints/src/lexibrarian/artifacts/design_file_serializer.md` + `design_file_parser.md` |
| Modify daemon behavior | `blueprints/src/lexibrarian/daemon/` |
| Add a tokenizer backend | `blueprints/src/lexibrarian/tokenizer/` |
| Change `lexi init` scaffolding | `blueprints/src/lexibrarian/init/scaffolder.md` |
| Change path utilities | `blueprints/src/lexibrarian/utils/paths.md` |
| Raise / handle project-not-found | `blueprints/src/lexibrarian/exceptions.md` |
| Add / modify concept wiki utilities | `blueprints/src/lexibrarian/wiki/` |
| Add / modify Stack Q&A features | `blueprints/src/lexibrarian/stack/` |
| Change cross-artifact search | `blueprints/src/lexibrarian/search.md` |

## Key Constraints

- `from __future__ import annotations` in **every** module
- pathspec pattern name is `"gitignore"` — NOT `"gitwildmatch"`
- Config: YAML via PyYAML, validated with Pydantic 2
- Output: `rich.console.Console` — no bare `print()`
- `baml_client/` is auto-generated — never edit it directly
- `crawler/engine.py` is partially broken: references `config.output`, `FileEntry`/`DirEntry`/`IandexData` types not yet defined — use `indexer/orchestrator.py` for `lexi index` instead

## Navigation Protocol

Before editing a file, read its design file in `blueprints/src/`. Keep design files updated when you change source files — the source is truth, the design file is the explanation.
